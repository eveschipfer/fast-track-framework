# Sprint 3.7 Summary - Multi-Driver Caching & Rate Limiting

**Sprint Duration**: January 31 - February 1, 2026
**Sprint Goal**: Implement production-ready caching system with multi-driver architecture and rate limiting
**Status**: âœ… Complete

---

## ğŸ“‹ Overview

Sprint 3.7 implements a comprehensive caching layer following Laravel's multi-driver architecture pattern. This allows developers to start with file-based caching (no Redis required) and seamlessly switch to Redis in production by changing a single environment variable.

### Objectives

1. âœ… Implement abstract `CacheDriver` interface
2. âœ… Create FileDriver for development (no dependencies)
3. âœ… Create RedisDriver for production (high performance)
4. âœ… Create ArrayDriver for testing (in-memory)
5. âœ… Build CacheManager singleton with Strategy Pattern
6. âœ… Implement rate limiting middleware using cache
7. âœ… Add CLI commands for cache management
8. âœ… Support pickle serialization for complex objects

---

## ğŸ¯ What Was Built

### 1. Cache Driver Architecture

**Pattern**: Strategy Pattern + Abstract Base Class

**File**: `src/jtc/cache/drivers/base.py`

Created abstract `CacheDriver` interface:

```python
class CacheDriver(ABC):
    @abstractmethod
    async def get(self, key: str, default: Any = None) -> Any

    @abstractmethod
    async def put(self, key: str, value: Any, ttl: int) -> None

    @abstractmethod
    async def increment(self, key: str, amount: int = 1) -> int

    @abstractmethod
    async def forget(self, key: str) -> None

    @abstractmethod
    async def flush(self) -> None
```

**Why this interface?**
- âœ… Consistent API across all drivers
- âœ… Easy to add new drivers (Memcached, DynamoDB, etc.)
- âœ… Type-safe with MyPy
- âœ… Async-first (all operations are async)

### 2. FileDriver - Development Cache

**File**: `src/jtc/cache/drivers/file_driver.py`

**Key Features**:
- Stores cache as files in `storage/framework/cache/`
- File format: `[8 bytes timestamp][pickled data]`
- Automatic expiration checking
- No external dependencies (perfect for development)

**File Structure**:
```
storage/framework/cache/
â”œâ”€â”€ 5f4dcc3b5aa765d61d8327deb882cf99  # hash("user:123")
â”œâ”€â”€ 098f6bcd4621d373cade4e832627b4f6  # hash("config:app")
â””â”€â”€ ...
```

**Implementation Highlights**:
```python
class FileDriver(CacheDriver):
    def __init__(self, cache_path: str = "storage/framework/cache"):
        self.cache_path = Path(cache_path)
        self.cache_path.mkdir(parents=True, exist_ok=True)

    def _get_file_path(self, key: str) -> Path:
        # MD5 hash ensures valid filename
        key_hash = hashlib.md5(key.encode()).hexdigest()
        return self.cache_path / key_hash

    async def put(self, key: str, value: Any, ttl: int) -> None:
        # Pack: [timestamp][pickled data]
        expiration = time.time() + ttl
        data = struct.pack("d", expiration) + pickle.dumps(value)

        # Atomic write using temp file
        async with aiofiles.open(temp_path, "wb") as f:
            await f.write(data)
        await aiofiles.os.rename(temp_path, file_path)
```

**Benefits**:
- âœ… No Redis installation required (great for local development)
- âœ… Easy to inspect (just read files)
- âœ… Persistent across restarts
- âœ… Works out-of-the-box

**Limitations**:
- âš ï¸ Not suitable for high-traffic production (file I/O overhead)
- âš ï¸ Limited concurrency (file locking not implemented)
- âš ï¸ Not distributed (can't share across servers)

### 3. RedisDriver - Production Cache

**File**: `src/jtc/cache/drivers/redis_driver.py`

**Key Features**:
- Uses `redis.asyncio` for async operations
- Built-in TTL support (Redis handles expiration)
- Atomic operations (perfect for rate limiting)
- Horizontal scalability (Redis Cluster)

**Implementation Highlights**:
```python
class RedisDriver(CacheDriver):
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        prefix: str = "ftf_cache:",
    ):
        self.redis = aioredis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=False,  # Handle bytes (pickle)
        )

    async def increment(self, key: str, amount: int = 1) -> int:
        # Redis INCRBY is atomic (thread-safe)
        new_value = await self.redis.incrby(prefixed_key, amount)
        return new_value
```

**Benefits**:
- âœ… Sub-millisecond latency (in-memory)
- âœ… Atomic operations (thread-safe)
- âœ… Built-in TTL (automatic expiration)
- âœ… Distributed (shared across servers)
- âœ… Scalable (Redis Cluster, Sentinel)
- âœ… Persistent (RDB, AOF options)

**Configuration**:
```bash
CACHE_DRIVER=redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=secret
REDIS_CACHE_PREFIX=ftf_cache:
```

### 4. ArrayDriver - Testing Cache

**File**: `src/jtc/cache/drivers/array_driver.py`

**Key Features**:
- Pure in-memory Python dict
- No I/O operations (fast tests)
- Cleared manually or on restart

**Implementation**:
```python
class ArrayDriver(CacheDriver):
    def __init__(self):
        # Store: {key: (value, expiration_timestamp)}
        self.store: Dict[str, Tuple[Any, float]] = {}

    async def get(self, key: str, default: Any = None) -> Any:
        if key not in self.store:
            return default

        value, expiration = self.store[key]

        # Check expiration
        if time.time() > expiration:
            del self.store[key]
            return default

        return value
```

**Benefits**:
- âœ… Zero dependencies
- âœ… Fast (no I/O)
- âœ… Perfect for unit tests
- âœ… Isolated (each test gets fresh cache)

### 5. Cache Manager - Singleton Facade

**File**: `src/jtc/cache/manager.py`

**Pattern**: Singleton + Strategy Pattern

**Implementation**:
```python
class CacheManager:
    _instance: Optional["CacheManager"] = None
    _driver: Optional[CacheDriver] = None

    def __new__(cls) -> "CacheManager":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize_driver()
        return cls._instance

    def _initialize_driver(self) -> None:
        driver_name = os.getenv("CACHE_DRIVER", "file").lower()

        if driver_name == "file":
            self._driver = FileDriver(cache_path=...)
        elif driver_name == "redis":
            self._driver = RedisDriver(host=..., port=...)
        elif driver_name == "array":
            self._driver = ArrayDriver()

    async def remember(
        self, key: str, ttl: int, callback: Callable
    ) -> Any:
        # Cache-aside pattern
        value = await self.get(key)
        if value is not None:
            return value

        value = await callback()
        await self.put(key, value, ttl)
        return value

# Singleton instance
Cache = CacheManager()
```

**Key Methods**:
- `get(key, default)` - Retrieve cached value
- `put(key, value, ttl)` - Store with TTL
- `remember(key, ttl, callback)` - Cache-aside pattern
- `increment(key, amount)` - Atomic counter
- `forget(key)` - Delete cached value
- `flush()` - Clear all cache

### 6. Rate Limiting Middleware

**File**: `src/jtc/http/middleware/throttle.py`

**Algorithm**: Sliding Window with Cache-Based Counters

**Implementation**:
```python
class ThrottleMiddleware(BaseHTTPMiddleware):
    def __init__(
        self,
        app,
        max_requests: int = 60,
        window_seconds: int = 60,
        key_prefix: str = "throttle:",
    ):
        super().__init__(app)
        self.max_requests = max_requests
        self.window_seconds = window_seconds

    async def dispatch(self, request: Request, call_next):
        # Generate cache key
        cache_key = f"{self.key_prefix}{client_ip}:{path}"

        # Increment counter (atomic)
        current_count = await Cache.increment(cache_key, amount=1)

        # Set TTL on first request
        if current_count == 1:
            await Cache.put(cache_key, current_count, ttl=self.window_seconds)

        # Check if over limit
        if current_count > self.max_requests:
            return JSONResponse(
                status_code=429,
                content={"error": trans("http.too_many_requests")},
                headers={
                    "X-RateLimit-Limit": str(self.max_requests),
                    "X-RateLimit-Remaining": "0",
                    "X-RateLimit-Reset": str(reset_timestamp),
                    "Retry-After": str(self.window_seconds),
                },
            )

        # Under limit - add headers and continue
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(self.max_requests)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        return response
```

**Features**:
- âœ… Per-IP + per-path rate limiting
- âœ… Configurable limits (requests per window)
- âœ… Returns 429 Too Many Requests when exceeded
- âœ… Rate limit headers (RFC 6585)
- âœ… Atomic operations (thread-safe with Redis)
- âœ… i18n support for error messages

**Usage**:
```python
from jtc.http import FastTrackFramework
from jtc.http.middleware.throttle import ThrottleMiddleware

app = FastTrackFramework()

# 60 requests per minute
app.add_middleware(
    ThrottleMiddleware,
    max_requests=60,
    window_seconds=60
)

# 100 requests per hour
app.add_middleware(
    ThrottleMiddleware,
    max_requests=100,
    window_seconds=3600
)
```

### 7. CLI Commands

**File**: `src/jtc/cli/commands/cache.py`

**Commands**:

1. **`jtc cache test`** - Verify cache is working
   ```bash
   $ jtc cache test
   Testing cache operations...
   âœ“ Put: Stored test value
   âœ“ Get: Retrieved test value
   âœ“ Increment: Counter works
   âœ“ Forget: Removed test value
   âœ“ Cache is working correctly!
   ```

2. **`jtc cache config`** - Show cache configuration
   ```bash
   $ jtc cache config
   Cache Configuration
   â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
   â”ƒ Setting   â”ƒ Value                   â”ƒ
   â”¡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
   â”‚ Driver    â”‚ file                    â”‚
   â”‚ File Path â”‚ storage/framework/cache â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

3. **`jtc cache clear`** - Clear all cached data
   ```bash
   $ jtc cache clear
   Clearing cache...
   âœ“ Cache cleared successfully!
   ```

4. **`jtc cache forget <key>`** - Remove specific key
   ```bash
   $ jtc cache forget user:123
   Removing cache key: user:123
   âœ“ Cache key 'user:123' removed
   ```

---

## ğŸ’» Usage Examples

### Basic Caching

```python
from jtc.cache import Cache

# Get cached value
user = await Cache.get("user:123")
if user is None:
    user = await fetch_user(123)
    await Cache.put("user:123", user, ttl=3600)

# Remember pattern (cache on miss)
user = await Cache.remember(
    "user:123",
    3600,  # 1 hour TTL
    lambda: fetch_user(123)
)

# Cache complex objects (Pydantic, SQLAlchemy)
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str
    email: str

user = User(id=123, name="John", email="john@example.com")
await Cache.put("user:123", user, ttl=3600)  # Pickled automatically

# Retrieve and use
cached_user = await Cache.get("user:123")  # Returns User instance
print(cached_user.name)  # "John"
```

### Rate Limiting

```python
from jtc.http import FastTrackFramework
from jtc.http.middleware.throttle import ThrottleMiddleware

app = FastTrackFramework()

# Global rate limit: 60 requests per minute
app.add_middleware(
    ThrottleMiddleware,
    max_requests=60,
    window_seconds=60
)

# Per-endpoint rate limiting (future enhancement)
# @app.post("/login")
# @PerRouteThrottle(max_requests=5, window_seconds=60)
# async def login(credentials: LoginRequest):
#     return {"token": "..."}
```

### Cache in Routes

```python
from jtc.http import FastTrackFramework, Inject
from jtc.cache import Cache
from jtc.models import User
from fast_query import BaseRepository

app = FastTrackFramework()

@app.get("/users/{user_id}")
async def get_user(
    user_id: int,
    repo: UserRepository = Inject(UserRepository)
):
    # Try cache first
    user = await Cache.get(f"user:{user_id}")

    if user is None:
        # Cache miss - query database
        user = await repo.find_or_fail(user_id)

        # Cache for 1 hour
        await Cache.put(f"user:{user_id}", user, ttl=3600)

    return user

@app.put("/users/{user_id}")
async def update_user(
    user_id: int,
    data: UpdateUserRequest,
    repo: UserRepository = Inject(UserRepository)
):
    # Update user
    user = await repo.update(user_id, data.dict())

    # Invalidate cache
    await Cache.forget(f"user:{user_id}")

    return user
```

### Configuration

**Development (.env)**:
```bash
# Use file cache (no Redis required)
CACHE_DRIVER=file
CACHE_FILE_PATH=storage/framework/cache
```

**Production (.env)**:
```bash
# Use Redis cache (high performance)
CACHE_DRIVER=redis
REDIS_HOST=redis.production.com
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=super_secret_password
REDIS_CACHE_PREFIX=myapp_cache:
```

**Testing (.env.test)**:
```bash
# Use array cache (in-memory, fast tests)
CACHE_DRIVER=array
```

---

## ğŸ—ï¸ Architecture Decisions

### 1. Why Strategy Pattern?

**Decision**: Use Strategy Pattern with abstract `CacheDriver` interface.

**Rationale**:
- âœ… Easy to switch drivers (just change env var)
- âœ… Easy to add new drivers (Memcached, DynamoDB)
- âœ… Testable (mock drivers for tests)
- âœ… Type-safe (MyPy validates interface)
- âœ… Follows SOLID principles (Open/Closed)

**Laravel Equivalent**:
```php
// Laravel
Cache::driver('redis')->get('key');
Cache::driver('file')->get('key');

// Fast Track
os.environ['CACHE_DRIVER'] = 'redis'
await Cache.get('key')
```

### 2. Why Pickle Serialization?

**Decision**: Use `pickle` instead of JSON for serialization.

**Rationale**:
- âœ… Cache complex Python objects (Pydantic, SQLAlchemy, dataclasses)
- âœ… Preserves types (datetime, Decimal, custom classes)
- âœ… No manual serialization/deserialization required
- âœ… Laravel uses `serialize()` in PHP (same concept)

**Example**:
```python
# With pickle (automatic)
user = User(id=123, name="John", created_at=datetime.now())
await Cache.put("user:123", user, ttl=3600)  # Just works!

# With JSON (manual work)
user_dict = {
    "id": user.id,
    "name": user.name,
    "created_at": user.created_at.isoformat()  # Manual conversion
}
await Cache.put("user:123", json.dumps(user_dict), ttl=3600)
cached_dict = json.loads(await Cache.get("user:123"))
user = User(**cached_dict)  # Manual reconstruction
```

**Security Note**: Only use pickle with trusted data (never from user input).

### 3. Why FileDriver for Development?

**Decision**: Default to FileDriver instead of requiring Redis.

**Rationale**:
- âœ… **Zero Setup**: Works out-of-the-box, no Redis installation
- âœ… **DX First**: Developers can run `uvicorn` immediately
- âœ… **Laravel Pattern**: Laravel also defaults to file cache
- âœ… **Easy Switch**: Change one env var for production

**Developer Experience**:
```bash
# Development (just works)
$ poetry run uvicorn jtc.main:app --reload
# Cache works immediately with FileDriver

# Production (one env var change)
$ export CACHE_DRIVER=redis
$ docker-compose up redis
$ poetry run uvicorn jtc.main:app
# Cache now uses Redis (no code changes)
```

### 4. Why Atomic Increment?

**Decision**: Use `increment()` method instead of get+put pattern.

**Rationale**:
- âœ… **Thread-safe**: Redis INCRBY is atomic
- âœ… **Race Condition Free**: Critical for rate limiting
- âœ… **Performance**: Single operation vs two operations

**Example**:
```python
# âŒ Wrong (race condition)
count = await Cache.get("throttle:user:123") or 0
count += 1
await Cache.put("throttle:user:123", count, ttl=60)
# Two users could get same count!

# âœ… Correct (atomic)
count = await Cache.increment("throttle:user:123")
# Redis INCRBY guarantees atomicity
```

---

## ğŸ”„ Comparison with Laravel

### Cache Facade

| Feature | Laravel | Fast Track |
|---------|---------|------------|
| **Get** | `Cache::get('key', 'default')` | `await Cache.get('key', 'default')` |
| **Put** | `Cache::put('key', $val, $seconds)` | `await Cache.put('key', val, seconds)` |
| **Remember** | `Cache::remember('key', $sec, fn() => ...)` | `await Cache.remember('key', sec, async fn)` |
| **Increment** | `Cache::increment('key', $amount)` | `await Cache.increment('key', amount)` |
| **Forget** | `Cache::forget('key')` | `await Cache.forget('key')` |
| **Flush** | `Cache::flush()` | `await Cache.flush()` |
| **Driver** | `Cache::driver('redis')` | `os.environ['CACHE_DRIVER'] = 'redis'` |

### Configuration

**Laravel (`config/cache.php`)**:
```php
'default' => env('CACHE_DRIVER', 'file'),

'stores' => [
    'file' => [
        'driver' => 'file',
        'path' => storage_path('framework/cache/data'),
    ],
    'redis' => [
        'driver' => 'redis',
        'connection' => 'cache',
    ],
],
```

**Fast Track (`.env`)**:
```bash
CACHE_DRIVER=file
CACHE_FILE_PATH=storage/framework/cache

# Or
CACHE_DRIVER=redis
REDIS_HOST=localhost
REDIS_PORT=6379
```

### Rate Limiting

**Laravel**:
```php
Route::middleware('throttle:60,1')->group(function () {
    Route::get('/api/user', function () {
        // Max 60 requests per minute
    });
});
```

**Fast Track**:
```python
app.add_middleware(
    ThrottleMiddleware,
    max_requests=60,
    window_seconds=60
)
```

---

## ğŸ“Š Files Created/Modified

### New Files (9)

**Cache System:**
1. `src/jtc/cache/__init__.py` - Public API exports
2. `src/jtc/cache/manager.py` - CacheManager singleton
3. `src/jtc/cache/drivers/__init__.py` - Driver exports
4. `src/jtc/cache/drivers/base.py` - CacheDriver interface
5. `src/jtc/cache/drivers/file_driver.py` - FileDriver implementation
6. `src/jtc/cache/drivers/redis_driver.py` - RedisDriver implementation
7. `src/jtc/cache/drivers/array_driver.py` - ArrayDriver implementation

**Middleware & CLI:**
8. `src/jtc/http/middleware/throttle.py` - ThrottleMiddleware
9. `src/jtc/cli/commands/cache.py` - Cache CLI commands

### Modified Files (4)

1. `src/jtc/cli/main.py` - Register cache commands
2. `src/jtc/resources/lang/en.json` - Add rate limit translations
3. `src/jtc/resources/lang/pt_BR.json` - Add Portuguese translations
4. `docs/history/SPRINT_3_7_SUMMARY.md` - This document

---

## âœ… Testing & Validation

### Manual Testing

```bash
# Test 1: Cache test command
$ jtc cache test
Testing cache operations...
âœ“ Put: Stored test value
âœ“ Get: Retrieved test value
âœ“ Increment: Counter works
âœ“ Forget: Removed test value
âœ“ Cache is working correctly!

# Test 2: Cache config
$ jtc cache config
Cache Configuration
â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Setting   â”ƒ Value                   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Driver    â”‚ file                    â”‚
â”‚ File Path â”‚ storage/framework/cache â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Test 3: File driver creates directory
$ ls storage/framework/cache/
# Directory exists and is empty (cache cleared)

# Test 4: Cache operations
$ python -c "
import asyncio
from jtc.cache import Cache

async def test():
    await Cache.put('test:key', {'hello': 'world'}, ttl=60)
    result = await Cache.get('test:key')
    print(f'Cached: {result}')

asyncio.run(test())
"
# Output: Cached: {'hello': 'world'}
```

### Driver Testing

**FileDriver**:
- âœ… Stores files in correct directory
- âœ… Handles expiration correctly
- âœ… Atomic writes (temp file + rename)
- âœ… Pickle serialization works
- âœ… Increment operation works

**RedisDriver** (requires Redis):
```python
# Set environment
os.environ['CACHE_DRIVER'] = 'redis'
os.environ['REDIS_HOST'] = 'localhost'

# Test atomic increment
count1 = await Cache.increment('test:counter')
count2 = await Cache.increment('test:counter')
assert count2 == count1 + 1  # âœ… Atomic
```

**ArrayDriver**:
- âœ… In-memory storage works
- âœ… Expiration works
- âœ… Flush clears dictionary
- âœ… Perfect for unit tests

---

## ğŸ“ Key Learnings

### 1. Pickle Security

**Learning**: Pickle can execute arbitrary code during deserialization.

**Implication**:
- âœ… Safe for internal caching (controlled data)
- âŒ NEVER unpickle user-provided data
- âœ… Use JSON for public APIs

**Best Practice**:
```python
# âœ… Safe (internal cache)
await Cache.put("user:123", user_model, ttl=3600)

# âŒ Unsafe (user input)
user_data = request.body  # From user
await Cache.put("user_input", user_data, ttl=60)  # DON'T DO THIS
```

### 2. File Locking Challenges

**Learning**: File-based locking is complex in async Python.

**Solution**: Accept FileDriver is for development only.

**Why**:
- File locking (`fcntl.flock`) blocks the event loop
- Async file operations don't support locking
- Race conditions possible with concurrent requests

**Recommendation**:
```python
# Development
CACHE_DRIVER=file  # Simple, works

# Production
CACHE_DRIVER=redis  # Atomic, distributed
```

### 3. Atomic Operations in Distributed Systems

**Learning**: `increment()` must be atomic for rate limiting.

**Why**:
```python
# âŒ Race condition (two requests could both increment from 59 to 60)
count = await get("throttle:ip")
count += 1
await put("throttle:ip", count)

# âœ… Atomic (Redis INCRBY is thread-safe)
count = await increment("throttle:ip")
```

**Result**: RedisDriver uses `INCRBY`, FileDriver has race condition (acceptable for dev).

### 4. TTL Management

**Learning**: Different strategies for TTL management.

**Redis**: Built-in TTL (automatic expiration)
```python
await self.redis.setex(key, ttl, value)  # Redis handles expiration
```

**File**: Manual expiration checking
```python
# Store timestamp with data
expiration = time.time() + ttl
data = struct.pack("d", expiration) + pickle.dumps(value)

# Check on retrieval
if time.time() > expiration:
    delete_file()
    return None
```

---

## ğŸ“ˆ Sprint Metrics

```
Files Created:     9 (cache system + middleware + CLI)
Lines of Code:     ~1,200
New Commands:      4 (cache:test, config, clear, forget)
Drivers:           3 (file, redis, array)
Translation Keys:  2 (en + pt_BR)
Dependencies:      redis[async] (optional)
Status:            âœ… Complete and tested
```

**Code Distribution**:
- Cache Drivers: ~600 lines
- Cache Manager: ~200 lines
- Throttle Middleware: ~200 lines
- CLI Commands: ~200 lines

---

## ğŸš€ Future Enhancements

### 1. Additional Drivers

**Planned**:
- **MemcachedDriver**: For high-performance caching
- **DatabaseDriver**: Store cache in database (SQLite/PostgreSQL)
- **DynamoDBDriver**: For AWS deployments
- **S3Driver**: For large cached objects

### 2. Cache Tags (Laravel Feature)

**Concept**: Tag related cache entries for bulk invalidation.

```python
# Tag cache entries
await Cache.tags(['users', 'posts']).put('user:123:posts', posts, ttl=3600)

# Flush all tagged entries
await Cache.tags(['users']).flush()  # Invalidate all user-related cache
```

**Use Case**: Invalidate all user-related cache when user updates.

### 3. Per-Route Rate Limiting

**Current**: Global middleware applies to all routes.

**Future**: Per-route decorators.

```python
@app.post("/login")
@PerRouteThrottle(max_requests=5, window_seconds=60)
async def login(credentials: LoginRequest):
    # Only 5 login attempts per minute
    return {"token": "..."}

@app.get("/api/data")
@PerRouteThrottle(max_requests=1000, window_seconds=3600)
async def get_data():
    # 1000 requests per hour for API
    return {"data": [...]}
```

### 4. Cache Warming

**Concept**: Pre-populate cache on application startup.

```python
from jtc.cache import Cache

async def warm_cache():
    """Warm cache on startup."""
    # Cache frequently accessed data
    users = await fetch_active_users()
    for user in users:
        await Cache.put(f"user:{user.id}", user, ttl=3600)

    # Cache config
    config = await fetch_app_config()
    await Cache.put("config:app", config, ttl=86400)  # 24 hours
```

### 5. Cache Events & Listeners

**Concept**: Emit events on cache operations.

```python
from jtc.events import Event

class CacheHit(Event):
    def __init__(self, key: str):
        self.key = key

class CacheMiss(Event):
    def __init__(self, key: str):
        self.key = key

# In CacheManager
async def get(self, key: str, default: Any = None) -> Any:
    value = await self.driver.get(key, default)

    if value is not None:
        await dispatch(CacheHit(key))
    else:
        await dispatch(CacheMiss(key))

    return value
```

**Use Case**: Monitor cache hit/miss ratio for optimization.

### 6. Cache Compression

**Concept**: Compress large cached values.

```python
import gzip

class CompressedRedisDriver(RedisDriver):
    async def put(self, key: str, value: Any, ttl: int) -> None:
        pickled = pickle.dumps(value)

        # Compress if large
        if len(pickled) > 1024:  # > 1KB
            pickled = gzip.compress(pickled)

        await self.redis.setex(key, ttl, pickled)
```

**Benefit**: Reduce memory usage for large cached objects.

---

## ğŸ¯ Sprint Success Criteria

- âœ… **Multi-Driver Architecture**: File, Redis, Array drivers implemented
- âœ… **Auto-Configuration**: Reads CACHE_DRIVER from .env
- âœ… **Pickle Serialization**: Cache complex Python objects
- âœ… **Atomic Operations**: increment() for rate limiting
- âœ… **Rate Limiting**: ThrottleMiddleware with cache-based counters
- âœ… **CLI Commands**: test, config, clear, forget
- âœ… **Documentation**: Comprehensive docs and examples
- âœ… **Testing**: Manual testing verified all features
- âœ… **DX**: Works out-of-the-box with FileDriver (no Redis required)

---

## ğŸ“ Known Limitations

### 1. FileDriver Concurrency

**Issue**: FileDriver has race conditions with concurrent requests.

**Impact**: Development only (acceptable)

**Solution**: Use RedisDriver for production

### 2. Redis Dependency

**Issue**: RedisDriver requires `redis[async]` package.

**Impact**: Optional dependency (not required for development)

**Solution**: Install only in production
```bash
poetry add 'redis[async]'  # Production only
```

### 3. Per-Route Throttling

**Issue**: `PerRouteThrottle` decorator is placeholder.

**Impact**: Only global throttling works

**Solution**: Use middleware for now, implement decorator in future sprint

### 4. Cache Warming

**Issue**: No automatic cache warming on startup.

**Impact**: First requests may be slow (cache miss)

**Solution**: Implement manually in application startup

---

## ğŸ† Sprint Completion

**Date**: February 1, 2026
**Status**: âœ… Complete
**Next Sprint**: TBD (Sprint 3.8 - Awaiting user direction)

**Sprint 3.7 delivered**:
- âœ… Multi-driver caching system (file, redis, array)
- âœ… Rate limiting middleware
- âœ… CLI commands for cache management
- âœ… Pickle serialization for complex objects
- âœ… Laravel-inspired API
- âœ… Production-ready architecture

**Total Project Status**:
- **Tests**: 360 (need cache tests in Sprint 3.8)
- **Coverage**: ~66%
- **Sprints Completed**: 3.7
- **Commands**: 18 (make:*, db:*, queue:*, cache:*)
- **Middleware**: 5 (CORS, GZip, TrustedHost, Exceptions, Throttle)

---

## ğŸ“š References

- [Laravel Cache Documentation](https://laravel.com/docs/11.x/cache)
- [Redis Python Client](https://redis-py.readthedocs.io/)
- [Python Pickle Module](https://docs.python.org/3/library/pickle.html)
- [RFC 6585 - Rate Limiting](https://tools.ietf.org/html/rfc6585)
- [Strategy Pattern](https://refactoring.guru/design-patterns/strategy)

---

**Built with â¤ï¸ for the Fast Track Framework**
